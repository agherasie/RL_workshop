{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "### Introduction\n",
    "Today, you'll learn about QLearning using the gym api.\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments.\n",
    "\n",
    "Reinforcement learning is a subset of Artificial Intelligence. Our AI will learn by playing the same game over and over again until it learns what it must do in order to win.\n",
    "\n",
    "In this workshop, you will:\n",
    "\n",
    "1. Learn about Q Tables\n",
    "2. Setup a gym environment\n",
    "3. Train an AI to solve the FrozenLake environment (https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "\n",
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygame gym numpy matplotlib\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of Q-Learning: \n",
    "\n",
    "![Example of a Q-Learning Environment](./images/example.png)\n",
    "\n",
    "the illustrations in this notebook come from here: (https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/#:~:text=Q%2DTable%20is%20just%20a,at%20each%20non%2Dedge%20tile.)\n",
    "\n",
    "A game where the AI must get to the ‚ÄúEnd‚Äù coordinates on the map in the shortest amount of time while avoiding the bombs and collecting the bonuses.\n",
    "\n",
    "A Q-Table for this environment could be visualized like this. \n",
    "\n",
    "![Example of a Q-Table](./images/qtable.png)\n",
    "\n",
    "With all of the possible actions listed horizontally and all of the possible states listed vertically.\n",
    "\n",
    "If you wish to visualise this in python, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty table\n",
    "\n",
    "def initQtable(x,y):\n",
    "## Create an array of zeroes of shape 5x4\n",
    "## correction: qTable = np.zeros((x, y))\n",
    "    return None\n",
    "\n",
    "qTable = initQtable(5, 4)\n",
    "print(\"Q-Table:\\n\" + str(qTable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the AI receives a **reward** each time it **acts**:\\\n",
    "Here, the AI would likely receive a **negative reward** (-1) if it chooses to move to the right (because there's a bomb)\\\n",
    "However, if it went anywhere else, it would probably receive a **neutral reward** (0), since it would end up on blank squares.\\\n",
    "Assuming it decided to go to the right, the AI would encounter a bomb and the value for [Start, Move_Right] would decrease.\\\n",
    "In order to decide what its new value will be, we use the following formula:\n",
    "\n",
    "![QFormula](./images/qformula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation:\n",
    "\n",
    "**Q<sup>new</sup>(s<sub>t</sub>, a<sub>t</sub>)** = the new value for the current State and Action on our Qtable\\\n",
    "**Q(s<sub>t</sub>, a<sub>t</sub>)** = the old value for the current State and Action on our QTable\\\n",
    "**Œ± (learning rate)** = a constant value we use for our algorithm's learning speed (usually a number between 0.5 and 0.05)\\\n",
    "**r<sub>t</sub> (reward)** = the reward received (as per the example above, -1 if you hit a bomb)\\\n",
    "**ùõæ (discount factor)** = another constant value we use to determine how good of a memory our AI has\\\n",
    "**max Q<sup>new</sup>(s<sub>t+1</sub>, a)** = the value of the most profitable action at the new state\n",
    "\n",
    "Let's try to implement this formula as a python function:\\\n",
    "(You can use the `learning_rate` and `discount_factor` constants in your formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "DISCOUNT_RATE = 0.99\n",
    "\n",
    "def qFunc(qTable, state, action, reward, newState):\n",
    "    ## enter your code here:\n",
    "    Q_New = None\n",
    "    ##\n",
    "    # correction: optimal_future_value = max(qTable[newState])\n",
    "    # correction: Q_New = qTable[state, action] + LEARNING_RATE * (reward + DISCOUNT_RATE * optimal_future_value - qTable[state, action])\n",
    "    return Q_New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If qTable[0, 1]'s value below is `-0.05`, congrats: you have successfully implemented the formula !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qTable = initQtable(5,4)\n",
    "\n",
    "qTable[0, 1] = qFunc(qTable=qTable, state=0, action=1, reward=-1, newState=3)\n",
    "\n",
    "print(\"Q-Table after action:\\n\" + str(qTable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the GYM Environment\n",
    "\n",
    "Let's get to the fun part:\n",
    "Read the documentation for FrozenLake here: https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\n",
    "and find out how to load the environment.\\\n",
    "\n",
    "For now, we will set the `is_slippery` variable to `False` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code to load and make the FrozenLake environment:\n",
    "env = None\n",
    "# correction: env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(env):\n",
    "    ## the action should be equal to a random number between 0 and the number of possible actions (find this value inside env)\n",
    "    return None\n",
    "    # correction: random.randint(0, env.action_space.n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(return_info=True)\n",
    "\n",
    "# Performing an action\n",
    "action = random_action(env)\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "# Displaying the first frame of the game\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "# Printing game info\n",
    "print(f\"actions: {env.action_space.n}\\nstates: {env.observation_space.n}\")\n",
    "print(f\"Current state: {observation}\")\n",
    "\n",
    "# Closing the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, there are **4 possible actions** for each of the **16 possible states**.\\\n",
    "Feel free to play around with the code above to get a better understanding of the API.\n",
    "\n",
    "## 3. Solving the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_loop(env, qTable, action):\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    ## Use the qFunc() function you wrote above to change the Q-Table\n",
    "    ## All of the arguments needed for the qFunc() function can be found inside this loop\n",
    "    qTable[state, action] = None\n",
    "    # correction : qFunc(state=state, action=action, newState=new_state, qTable=qTable, reward=reward)\n",
    "    ##\n",
    "    return qTable, new_state, done, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "qTable = initQtable((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "state, info = env.reset(return_info=True)\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = random_action(env)\n",
    "    qTable, state, done, reward = game_loop(env, qTable, action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to see if it works, we will launch the environment 1000 times and see how the Q-Table evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10000\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "qTable = initQtable((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    state, info = env.reset(return_info=True)\n",
    "    while (True):\n",
    "        # This time, we won't render the game each frame because it would take too long\n",
    "        action = random_action(env)\n",
    "        qTable, state, done, reward = game_loop(env, qTable, action)\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "# Printing the QTable result:\n",
    "for states in qTable:\n",
    "    for actions in states:\n",
    "        if (actions == max(states)):\n",
    "            print(\"\\033[4m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[0m\", end=\"\")\n",
    "        if (actions > 0):\n",
    "            print(\"\\033[92m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[00m\", end=\"\")\n",
    "        print(round(actions, 3), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! We now have a nice Q-Table that knows which action is best for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, the AI should probably try to explore all the different possibilities before it starts optimising its gain...\n",
    "In order to solve this problem, we can use the Epsilon-Greedy strategy!\n",
    "\n",
    "```\n",
    "epsilon = 0.9\n",
    "random = random()\n",
    "if (random > epsilon):\n",
    "    greedy_action()\n",
    "else:\n",
    "    random_action()\n",
    "```\n",
    "\n",
    "Try to implement this into the following function that we'll use to determine which action the AI will choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(epsilon, qTable, state, env):\n",
    "    ## write some code that returns either a random action or the best action\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "state, info = env.reset(return_info=True)\n",
    "while (True):\n",
    "    env.render()\n",
    "    ## we can give it an epsilon of \"0\" because we want it to always choose the most profitable state\n",
    "    action = chooseAction(0, qTable, state, env)\n",
    "    qTable, state, done, reward = game_loop(env, qTable, action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Displaying the last frame of the game\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, our AI should easily reach its goal !\n",
    "\n",
    "But we're not done...\\\n",
    "You might have noticed that when we load our environment, we give it a certain argument:\\\n",
    "`is_slippery=False`\n",
    "\n",
    "This argument makes the game far easier !\n",
    "If you want a real challenge, set it to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "qTable = initQtable((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Training the AI\n",
    "epsilon = 1.0\n",
    "for i in range(10000):\n",
    "    epsilon = max(epsilon - 0.0001, 0)\n",
    "    state, info = env.reset(return_info=True)\n",
    "    while (True):\n",
    "        action = chooseAction(epsilon, qTable, state, env)\n",
    "        qTable, state, done, reward = game_loop(env, qTable, action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Testing the AI\n",
    "wins = 0.0\n",
    "for i in range(100):\n",
    "    state, info = env.reset(return_info=True)\n",
    "    while (True):\n",
    "        action = chooseAction(0, qTable, state, env)\n",
    "        _, state, done, reward = game_loop(env, qTable, action)\n",
    "        if done:\n",
    "            # increment the number of wins if the AI was successful (you can use one of the variables above for this)\n",
    "            # ~2 lines of code:\n",
    "            # correction: if (reward > 0):\n",
    "            #               wins += 1\n",
    "            break\n",
    "\n",
    "print(f\"{round(wins / (i+1) * 100, 2)}% winrate\")\n",
    "print(wins)\n",
    "\n",
    "# Displaying the last frame of the game\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job ! You completed this workshop !\n",
    "If you want to continue in the wonderful world of reinforcement learning, you could try:\n",
    "- A custom map for Frozen Lake. For example: `gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True)`\n",
    "- A different environment from https://www.gymlibrary.ml/\n",
    "- Going **deeper** with https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "- Trying what you've learned with games you know and love:\n",
    "    - https://www.gymlibrary.ml/environments/atari/\n",
    "    - https://pypi.org/project/gym-super-mario-bros/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
